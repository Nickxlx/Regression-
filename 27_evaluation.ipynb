{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78cd630-c061-4e7c-aecc-38b9bbf7fcce",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "Answer-->R-squared in linear regression models is use to calculate accuracy of the model. It ranges from 0 to 1. \n",
    "\n",
    "R-squared is calculated by dividing the sum of squared residuals (SSR) by the total sum of squares (SST).\n",
    "\n",
    "    The formula for calculating R-squared is as follows:\n",
    "    R-squared = 1 - (SSR / SST)\n",
    "\n",
    "It reperesents that higher the R-squared value better is fit of the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf061cf-db21-47d6-9f85-353135a4939a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cf68d-69c4-4f96-915e-958f6b5490fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf55d-2853-4672-8310-6b7d5803e79d",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Answer-->Adjusted R-squared is a modified version of R-squared that takes into account the number of predictors or independent variables in a linear regression model. While R-squared measures the proportion of the variance in the dependent variable explained by the independent variables, adjusted R-squared considers the complexity of the model by penalizing the inclusion of unnecessary predictors.\n",
    "\n",
    "The formula for calculating adjusted R-squared is as follows:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12474d53-16bd-4dbf-8dc4-1e57ff4be264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cff152-6400-40bc-82ce-0ba2bd556be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f768324-3cf1-4913-9f75-49885f5fd03c",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Answer--> Adjusted R-squared is more appropriate to use when comparing and evaluating models with different numbers of predictors or independent variables. It helps in selecting the most suitable model by considering both the goodness of fit and the complexity of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08309c3-d7ec-43b2-ac18-ab6d0d474ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ad0c4-2d17-4b66-ad88-691e93636887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07132348-cea5-46ab-9be1-c0e7bbff713f",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "ANswer--> RMSE, MSE, and MAE are commonly used metrics in regression analysis to evaluate the cost of a regression model. They provide measures of the average prediction error and the goodness of fit between the predicted values and the actual values of the dependent variable.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is a measure of the average prediction error between the predicted values and the actual values. It is calculated by taking the square root of the mean of the squared differences between the predicted and actual values.\n",
    "\n",
    "    The formula for RMSE is as follows:\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "MSE is a measure of the average squared difference between the predicted values and the actual values. It is calculated by taking the mean of the squared differences between the predicted and actual values.\n",
    "\n",
    "    The formula for MSE is as follows:\n",
    "    MSE = (1/n) * Σ(y_actual - y_predicted)^2\n",
    "\n",
    "where n is the number of observations, y_actual represents the actual values of the dependent variable, and y_predicted represents the predicted values.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "MAE is a measure of the average absolute difference between the predicted values and the actual values. It is calculated by taking the mean of the absolute differences between the predicted and actual values.\n",
    "\n",
    "    The formula for MAE is as follows:\n",
    "    MAE = (1/n) * Σ|y_actual - y_predicted|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01863b6e-d027-4544-909b-4571b5087c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e4b3d-6668-4440-86b0-99728bdf83ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea331bd2-ad79-4106-a5bd-c8f902262ce9",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "Answer--> RMSE, MSE, and MAE are widely used evaluation metrics in regression analysis, and each metric has its own advantages and disadvantages\n",
    "\n",
    "Advantages of RMSE:\n",
    "1. Equation is differencial so it make suitable for optimizatio. \n",
    "\n",
    "2. It is in the same unit.\n",
    "\n",
    "Disadvantages of RMSE:\n",
    "1. Not robust to the outlies .\n",
    "\n",
    "Advantages of MSE:\n",
    "1. Equation is differencial so it make suitable for optimizatio.  \n",
    "\n",
    "2. It has only one local or global minima.\n",
    "\n",
    "Disadvantages of MSE:\n",
    "1. Units are squared and not are the same unit.\n",
    "\n",
    "2. Sensitivity to outliers.\n",
    "\n",
    "Advantages of MAE:\n",
    "1. Robust to outliers\n",
    "\n",
    "2. It is in the same unit.\n",
    "\n",
    "Disadvantages of MAE:\n",
    "1. Converges takes usualy more time.\n",
    "\n",
    "2. Non-differentiable at zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e2752-64ff-4d4e-9880-3b7b73fe7706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51839170-cf8a-42bd-9835-0803f339237b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd3ea260-6b0e-4c33-a4dd-7034e19f07c3",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "Answer-->\n",
    "Lasso regularization, also known as L1 regularization, is a technique used in linear regression to add a penalty term to the loss function. It aims to encourage sparsity in the model by shrinking the coefficients of less important predictors towards zero, effectively performing feature selection.\n",
    "\n",
    "Lasso regularization differs from Ridge regularization (L2 regularization) in the type of penalty term applied to the loss function.\n",
    "\n",
    "    Ridge regularization adds the squared sum of the coefficients (L2 norm) as the penalty term where as Lasso regularization adds the absolute sum of the coefficients (L1 norm) as the penalty term.\n",
    "    \n",
    "    \n",
    "When to use Lasso regularization:\n",
    "\n",
    "    1 When feature selection is desired and there is a large number of predictors, especially when it is suspected that many of them are irrelevant or redundant.\n",
    "    \n",
    "    2 When there is a need to interpret the model and identify the most important predictors.\n",
    "    \n",
    "    3 When dealing with high-dimensional datasets or situations where sparsity is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c7cf1-0e47-40b2-ba01-8ab99b2023d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9174a1-161c-4420-8cce-df66c66f541d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca41abe0-2dad-40bb-b27a-edb7c5d1515f",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "Answer--> Regularized linear models, such as Ridge regression and Lasso regression, help prevent overfitting in machine learning by adding a penalty term to the loss function. This penalty term discourages the model from fitting the training data too closely, leading to more generalized and robust models.\n",
    "\n",
    "Here's an example to illustrate how regularized linear models prevent overfitting:\n",
    "\n",
    "    Let's say we have a dataset with a single input feature, x, and a target variable, y. We want to fit a linear regression model to this data. We have a limited number of training samples, and we suspect that the relationship between x and y may not be strictly linear.\n",
    "\n",
    "    Without regularization, a traditional linear regression model aims to minimize the sum of squared errors between the predicted and actual values on the training data. In this case, the model may try to fit the data points very closely, potentially capturing noise and outliers in the training set. This can lead to overfitting, where the model performs well on the training data but fails to generalize to unseen data.\n",
    "\n",
    "To prevent overfitting, we can introduce regularization techniques such as Ridge regression or Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae988f-c85b-475d-835d-37dcdaf757fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a28560-d588-41e5-80d2-e3f5fed0b410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d669329-c5c8-4e6a-a947-ee160a5b4548",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "Answer--> Regularized linear models have limitations in terms of their linearity assumption, handling large feature spaces, sensitivity to feature scaling interpretability, and the availability of alternative modeling techniques. It is essential to consider these limitations and carefully assess the suitability of regularized linear models for a given regression analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8587379a-9fbf-484a-9e06-04786cf9d928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90676573-baf9-47c6-a186-4f812bb0fa7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e5a208-7f36-4c3f-8dfa-40c6758ba3ab",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "\n",
    "Answer--> In this scenario, we have Model A with an RMSE (Root Mean Squared Error) of 10 and Model B with an MAE (Mean Absolute Error) of 8. Generally, a lower value of both RMSE and MAE indicates better model performance.\n",
    "\n",
    "Based on the given information, Model B has a lower MAE of 8 compared to Model A's RMSE of 10. Therefore, Model B can be considered as the better performer in terms of the chosen metric.\n",
    "\n",
    "The advantage of using MAE is that it provides a straightforward and interpretable measure of the average absolute difference between the predicted and actual values. It is less sensitive to outliers compared to RMSE, as it does not involve squaring the errors. This can be beneficial when the presence of outliers is a concern.\n",
    "\n",
    "Limitations of the mae of metric:\n",
    "\n",
    "    1 convergence of gradient decent takes more time \n",
    "    2 Non-Defferenciablee at Zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5797e-eefc-4f11-baa8-ba16dd77112c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b66302c-c160-4070-8c6b-6ae1e005d34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "654ae32a-8790-48a0-8b97-008b43a0a073",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "Answer-->To choose the better performer, we should need to consider the specific objectives and constraints of your analysis. If interpretability and feature selection are important, Model B with Lasso regularization may be preferred due to its ability to identify the most important predictors and potentially simplify the model. On the other hand, if the goal is to retain all predictors and reduce the impact of multicollinearity, Model A with Ridge regularization may be more appropriate.\n",
    "\n",
    "Trade-offs and limitations:\n",
    "\n",
    "    1 Ridge regularization does not perform exact feature selection, as it keeps all predictors in the model with small but non-zero coefficients. If strict feature selection is desired, Lasso regularization may be more appropriate.\n",
    "    \n",
    "    2 Lasso regularization's feature selection can be sensitive to correlated predictors, as it tends to select only one variable from a group of highly correlated variables. It may not always select the predictors that align with prior knowledge or domain expertise.\n",
    "    \n",
    "    3 The choice of regularization parameter (λ) is crucial in both methods. It requires careful tuning through techniques like cross-validation to find the optimal value that balances model complexity and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac5396-3624-44cf-8610-69bcdb756bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
